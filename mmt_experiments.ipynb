{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unlikely-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 09:56:18 | INFO | root | [1/2] Loaded 1 checkpoints in 1.4s\n",
      "2021-02-04 09:56:20 | INFO | root | [2/2] Decoder created in 2.3s\n",
      "== Running the same translation 5 times (after warm up) ==\n",
      "-- Warm Up --\n",
      "2021-02-04 09:56:22 | INFO | Transformer | reset_time = 0.062, tune_time = 0.000, decode_time = 1.136\n",
      "------\n",
      "- Original text: Companies and LSPs can translate their content with the ModernMT service in many languages directly on their production environment thanks to our simple RESTful API .\n",
      "------\n",
      "- Translated text: Le aziende e i LSP possono tradurre il loro contenuto con il servizio di modernità in molte lingue direttamente nel loro ambiente di produzione grazie alla nostra semplice API .\n",
      "------\n",
      "- Alignment: [(0, 0), (0, 1), (1, 2), (2, 4), (3, 5), (4, 6), (5, 7), (5, 8), (6, 9), (7, 10), (8, 11), (9, 13), (9, 14), (10, 12), (11, 15), (12, 16), (13, 17), (14, 18), (15, 19), (16, 20), (17, 22), (17, 23), (18, 21), (19, 24), (20, 24), (21, 25), (22, 26), (23, 27), (24, 28)]\n",
      "-- Timing --\n",
      "2021-02-04 09:56:23 | INFO | Transformer | reset_time = 0.061, tune_time = 0.000, decode_time = 1.129\n",
      "2021-02-04 09:56:24 | INFO | Transformer | reset_time = 0.061, tune_time = 0.000, decode_time = 1.129\n",
      "2021-02-04 09:56:25 | INFO | Transformer | reset_time = 0.062, tune_time = 0.000, decode_time = 1.128\n",
      "2021-02-04 09:56:26 | INFO | Transformer | reset_time = 0.061, tune_time = 0.000, decode_time = 1.130\n",
      "2021-02-04 09:56:27 | INFO | Transformer | reset_time = 0.061, tune_time = 0.000, decode_time = 1.130\n"
     ]
    }
   ],
   "source": [
    "!source test_speed_cpu.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unsigned-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "broad-operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "numeric-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.neuron\n",
    "\n",
    "from mmt import utils\n",
    "from mmt.checkpoint import CheckpointRegistry\n",
    "from mmt.decoder import Suggestion, ModelConfig, MMTDecoder\n",
    "from fairseq.sequence_generator import EnsembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "identical-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TEXT = 'Companies and LSPs can translate their content with the ModernMT service in many languages ' \\\n",
    "            'directly on their production environment thanks to our simple RESTful API .'\n",
    "MODEL_DIR = 'model'\n",
    "device=None\n",
    "test_text = TEST_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "precious-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig.load('model')\n",
    "builder = CheckpointRegistry.Builder()\n",
    "for name, checkpoint_path in config.checkpoints:\n",
    "    builder.register(name, checkpoint_path)\n",
    "checkpoints = builder.build(device)\n",
    "decoder = MMTDecoder(checkpoints, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "three-influence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Using [decoder.translate]: Le aziende e i LSP possono tradurre il loro contenuto con il servizio di modernità in molte lingue direttamente nel loro ambiente di produzione grazie alla nostra semplice API .\n",
      "- Using [decoder._decode]: Le aziende e i LSP possono tradurre il loro contenuto con il servizio di modernità in molte lingue direttamente nel loro ambiente di produzione grazie alla nostra semplice API .\n",
      "------\n",
      "Output of [decoder.translate] == [decoder._decode]\n"
     ]
    }
   ],
   "source": [
    "# A simple translation without using a tuner () \n",
    "trans_1 = decoder.translate('en', 'it', [test_text])[0]\n",
    "print(f'- Using [decoder.translate]: {trans_1.text}')\n",
    "trans_2 = decoder._decode('en', 'it', [test_text])[0]\n",
    "print(f'- Using [decoder._decode]: {trans_2.text}')\n",
    "print('------')\n",
    "print(f'Output of [decoder.translate] {\"==\" if trans_1.text == trans_2.text else \"!=\"} [decoder._decode]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-blocking",
   "metadata": {},
   "source": [
    "## Attempt to trace the `decoder` object\n",
    "\n",
    "#### Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "norman-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of [decoder] is <class 'mmt.decoder.MMTDecoder'>\n",
      "Type of [decoder._translator] is <class 'fairseq.sequence_generator.SequenceGenerator'>\n",
      "Type of [decoder._model] is <class 'fairseq.models.transformer.TransformerModel'>\n"
     ]
    }
   ],
   "source": [
    "print(f'Type of [decoder] is {type(decoder)}')\n",
    "print(f'Type of [decoder._translator] is {type(decoder._translator)}')\n",
    "print(f'Type of [decoder._model] is {type(decoder._model)}')\n",
    "\n",
    "# Tuner function is not used in these experiments\n",
    "# print(f'Type of `decoder._tuner`: {type(decoder._tuner)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-harbor",
   "metadata": {},
   "source": [
    "- Type of `decoder` is `MMTDecoder`, it is using `fairseq` classes internally\n",
    "- Internally, `decoder.translate('en', 'it', [TEST_TEXT])` ultimately calls `decoder._translator.generate([decoder._model], sample)` (`fairseq.sequence_generator.SequenceGenerator.generate(models, sample)` where `sample` is a tokenised text) but there is also `SequenceGenerator._generate(sample)` which uses `models` set within a `SequenceGenerator` constructor (which is also set correctly in the `MMTDecoder` constructor)\n",
    "\n",
    "#### Try to use `decoder._translator._generate(sample)` directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "controversial-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_encode, input_indexes, sentence_len = decoder._make_decode_batch([TEST_TEXT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "proof-realtor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text:\n",
      "Companies and LSPs can translate their content with the ModernMT service in many languages directly on their production environment thanks to our simple RESTful API .\n",
      "Encoded:\n",
      "{'net_input': {'src_tokens': tensor([[ 9055,  9632,   518,    22,  4764, 17506,   126,   127, 15470,   144,\n",
      "          2242,    54,    16, 21163,  6719, 29625,  1519,    18,   278,  3580,\n",
      "          2352,    34,   144,  1027,   887,  1933,    20,    99,  3127,  9896,\n",
      "         24193,  6082, 13779,    33,    15,     2]]), 'src_lengths': tensor([36])}}\n"
     ]
    }
   ],
   "source": [
    "print(f'Test text:\\n{test_text}')\n",
    "print(f'Encoded:\\n{test_text_encode}')\n",
    "# print(input_indexes)\n",
    "# print(sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "frozen-pollution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Using [decoder._translator._generate]: Le aziende e i LSP possono tradurre il loro contenuto con il servizio di modernità in molte lingue direttamente nel loro ambiente di produzione grazie alla nostra semplice API .\n",
      "------\n",
      "Output of [decoder.translate] == [decoder._translator._generate]\n"
     ]
    }
   ],
   "source": [
    "trans_3 = decoder._decode_without_explicit_model('en', 'it', [TEST_TEXT])[0]\n",
    "is_equal = \"==\" if trans_1.text == trans_3.text else \"!=\"\n",
    "print(f'- Using [decoder._translator._generate]: {trans_3.text}')\n",
    "print('------')\n",
    "print(f'Output of [decoder.translate] {is_equal} [decoder._translator._generate]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occupied-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorWrapper(torch.nn.Module):\n",
    "    def __init__(self, generator):\n",
    "        super(GeneratorWrapper, self).__init__()\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `_decode_from_sample` is a simple method I added which calls SequenceGenerator._generate\n",
    "        return self.generator._decode_from_sample(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-patient",
   "metadata": {},
   "source": [
    "***Attemp to JIT or Neuron trace here kills the browser due to a lot of data generated by the trace, to see those run a dedicated Python script in Terminal `python mmt_trace.py &> log.txt`. This generates a large log file!***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-tissue",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/torch_neuron_env/lib/python3.7/site-packages/fairseq/sequence_generator.py:229: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  int(self.max_len_a * src_len + self.max_len_b),\n"
     ]
    }
   ],
   "source": [
    "# Attempt to trace\n",
    "gen_wrapper = GeneratorWrapper(decoder)\n",
    "#jit_gen = torch.jit.trace(gen_wrapper, sample)\n",
    "#neuron_gen torch.neuron.trace(gen_wrapper, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-companion",
   "metadata": {},
   "source": [
    "### Trace the model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "international-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `_decode_from_sample` is a simple method I added which calls SequenceGenerator._generate\n",
    "        res = self.model.forward_encoder(x)\n",
    "        print(res)\n",
    "        return torch.Tensor([[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "italic-typing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/work/torch_neuron_env/lib/python3.7/site-packages/torch/tensor.py:593: RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  'incorrect results).', category=RuntimeWarning)\n",
      "/home/ubuntu/work/torch_neuron_env/lib/python3.7/site-packages/ipykernel_launcher.py:10: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EncoderOut(encoder_out=tensor([[[ 1.9738e-02,  5.7294e-02, -1.1316e-01,  ..., -1.5601e-01,\n",
      "           1.8267e-01, -2.2096e-02]],\n",
      "\n",
      "        [[ 6.7960e-04, -8.9271e-03, -1.1074e-01,  ..., -1.3071e-01,\n",
      "           2.8182e-02,  4.4077e-02]],\n",
      "\n",
      "        [[-4.3508e-02,  4.1582e-02, -1.0471e-01,  ..., -2.0630e-01,\n",
      "           6.9779e-02, -7.7151e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.2303e-01,  3.3949e-01, -3.5549e-01,  ..., -1.4366e-01,\n",
      "          -2.6553e-01, -1.7634e-01]],\n",
      "\n",
      "        [[ 1.7761e-02,  1.6541e-02,  2.6702e-02,  ..., -1.6961e-02,\n",
      "           3.9059e-02, -2.5640e-04]],\n",
      "\n",
      "        [[ 1.7764e-02,  1.6547e-02,  2.6700e-02,  ..., -1.6976e-02,\n",
      "           3.9067e-02, -2.5229e-04]]], grad_fn=<NativeLayerNormBackward>), encoder_padding_mask=tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]]), encoder_embedding=tensor([[[ 1.7729,  0.0546,  1.7963,  ..., -1.5727, -3.0264, -1.1402],\n",
      "         [-1.2333, -0.5330,  1.5719,  ..., -0.5679, -1.2131, -0.0988],\n",
      "         [-0.3549,  0.3646, -0.3198,  ..., -1.5547,  0.5346, -2.0776],\n",
      "         ...,\n",
      "         [-0.1532, -0.0905,  0.0056,  ..., -2.1383, -0.8436, -1.1194],\n",
      "         [-0.0879,  0.1654,  0.1021,  ...,  0.7207, -2.6127,  0.7723],\n",
      "         [-0.9114, -0.9360, -0.9302,  ..., -1.0435, -1.0247, -1.0042]]],\n",
      "       grad_fn=<MulBackward0>), encoder_states=None, src_tokens=None, src_lengths=None)]\n",
      "[EncoderOut(encoder_out=tensor([[[ 1.9738e-02,  5.7294e-02, -1.1316e-01,  ..., -1.5601e-01,\n",
      "           1.8267e-01, -2.2096e-02]],\n",
      "\n",
      "        [[ 6.7960e-04, -8.9271e-03, -1.1074e-01,  ..., -1.3071e-01,\n",
      "           2.8182e-02,  4.4077e-02]],\n",
      "\n",
      "        [[-4.3508e-02,  4.1582e-02, -1.0471e-01,  ..., -2.0630e-01,\n",
      "           6.9779e-02, -7.7151e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.2303e-01,  3.3949e-01, -3.5549e-01,  ..., -1.4366e-01,\n",
      "          -2.6553e-01, -1.7634e-01]],\n",
      "\n",
      "        [[ 1.7761e-02,  1.6541e-02,  2.6702e-02,  ..., -1.6961e-02,\n",
      "           3.9059e-02, -2.5640e-04]],\n",
      "\n",
      "        [[ 1.7764e-02,  1.6547e-02,  2.6700e-02,  ..., -1.6976e-02,\n",
      "           3.9067e-02, -2.5229e-04]]]), encoder_padding_mask=tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]]), encoder_embedding=tensor([[[ 1.7729,  0.0546,  1.7963,  ..., -1.5727, -3.0264, -1.1402],\n",
      "         [-1.2333, -0.5330,  1.5719,  ..., -0.5679, -1.2131, -0.0988],\n",
      "         [-0.3549,  0.3646, -0.3198,  ..., -1.5547,  0.5346, -2.0776],\n",
      "         ...,\n",
      "         [-0.1532, -0.0905,  0.0056,  ..., -2.1383, -0.8436, -1.1194],\n",
      "         [-0.0879,  0.1654,  0.1021,  ...,  0.7207, -2.6127,  0.7723],\n",
      "         [-0.9114, -0.9360, -0.9302,  ..., -1.0435, -1.0247, -1.0042]]]), encoder_states=None, src_tokens=None, src_lengths=None)]\n",
      "[EncoderOut(encoder_out=tensor([[[ 1.9738e-02,  5.7294e-02, -1.1316e-01,  ..., -1.5601e-01,\n",
      "           1.8267e-01, -2.2096e-02]],\n",
      "\n",
      "        [[ 6.7960e-04, -8.9271e-03, -1.1074e-01,  ..., -1.3071e-01,\n",
      "           2.8182e-02,  4.4077e-02]],\n",
      "\n",
      "        [[-4.3508e-02,  4.1582e-02, -1.0471e-01,  ..., -2.0630e-01,\n",
      "           6.9779e-02, -7.7151e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.2303e-01,  3.3949e-01, -3.5549e-01,  ..., -1.4366e-01,\n",
      "          -2.6553e-01, -1.7634e-01]],\n",
      "\n",
      "        [[ 1.7761e-02,  1.6541e-02,  2.6702e-02,  ..., -1.6961e-02,\n",
      "           3.9059e-02, -2.5640e-04]],\n",
      "\n",
      "        [[ 1.7764e-02,  1.6547e-02,  2.6700e-02,  ..., -1.6976e-02,\n",
      "           3.9067e-02, -2.5229e-04]]]), encoder_padding_mask=tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]]), encoder_embedding=tensor([[[ 1.7729,  0.0546,  1.7963,  ..., -1.5727, -3.0264, -1.1402],\n",
      "         [-1.2333, -0.5330,  1.5719,  ..., -0.5679, -1.2131, -0.0988],\n",
      "         [-0.3549,  0.3646, -0.3198,  ..., -1.5547,  0.5346, -2.0776],\n",
      "         ...,\n",
      "         [-0.1532, -0.0905,  0.0056,  ..., -2.1383, -0.8436, -1.1194],\n",
      "         [-0.0879,  0.1654,  0.1021,  ...,  0.7207, -2.6127,  0.7723],\n",
      "         [-0.9114, -0.9360, -0.9302,  ..., -1.0435, -1.0247, -1.0042]]]), encoder_states=None, src_tokens=None, src_lengths=None)]\n"
     ]
    }
   ],
   "source": [
    "model_wrapper = ModelWrapper(EnsembleModel([decoder._model]))\n",
    "jit_model = torch.jit.trace(model_wrapper, sample['net_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "stone-modeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelWrapper(\n",
       "  original_name=ModelWrapper\n",
       "  (model): RecursiveScriptModule(\n",
       "    original_name=EnsembleModel\n",
       "    (single_model): RecursiveScriptModule(\n",
       "      original_name=TransformerModel\n",
       "      (encoder): RecursiveScriptModule(\n",
       "        original_name=TransformerEncoder\n",
       "        (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "        (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
       "        (embed_positions): RecursiveScriptModule(original_name=SinusoidalPositionalEmbedding)\n",
       "        (layers): RecursiveScriptModule(\n",
       "          original_name=ModuleList\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=TransformerEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=TransformerEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=TransformerEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=TransformerEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (4): RecursiveScriptModule(\n",
       "            original_name=TransformerEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (5): RecursiveScriptModule(\n",
       "            original_name=TransformerEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): RecursiveScriptModule(\n",
       "        original_name=TransformerDecoder\n",
       "        (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "        (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
       "        (embed_positions): RecursiveScriptModule(original_name=SinusoidalPositionalEmbedding)\n",
       "        (layers): RecursiveScriptModule(\n",
       "          original_name=ModuleList\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=TransformerDecoderLayer\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=TransformerDecoderLayer\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=TransformerDecoderLayer\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=TransformerDecoderLayer\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (4): RecursiveScriptModule(\n",
       "            original_name=TransformerDecoderLayer\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (5): RecursiveScriptModule(\n",
       "            original_name=TransformerDecoderLayer\n",
       "            (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=MultiheadAttention\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "        )\n",
       "        (output_projection): RecursiveScriptModule(original_name=Linear)\n",
       "      )\n",
       "    )\n",
       "    (models): RecursiveScriptModule(\n",
       "      original_name=ModuleList\n",
       "      (0): RecursiveScriptModule(\n",
       "        original_name=TransformerModel\n",
       "        (encoder): RecursiveScriptModule(\n",
       "          original_name=TransformerEncoder\n",
       "          (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "          (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
       "          (embed_positions): RecursiveScriptModule(original_name=SinusoidalPositionalEmbedding)\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=ModuleList\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=TransformerEncoderLayer\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=TransformerEncoderLayer\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=TransformerEncoderLayer\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=TransformerEncoderLayer\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (4): RecursiveScriptModule(\n",
       "              original_name=TransformerEncoderLayer\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (5): RecursiveScriptModule(\n",
       "              original_name=TransformerEncoderLayer\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (decoder): RecursiveScriptModule(\n",
       "          original_name=TransformerDecoder\n",
       "          (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "          (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
       "          (embed_positions): RecursiveScriptModule(original_name=SinusoidalPositionalEmbedding)\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=ModuleList\n",
       "            (0): RecursiveScriptModule(\n",
       "              original_name=TransformerDecoderLayer\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (encoder_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (1): RecursiveScriptModule(\n",
       "              original_name=TransformerDecoderLayer\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (encoder_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (2): RecursiveScriptModule(\n",
       "              original_name=TransformerDecoderLayer\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (encoder_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (3): RecursiveScriptModule(\n",
       "              original_name=TransformerDecoderLayer\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (encoder_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (4): RecursiveScriptModule(\n",
       "              original_name=TransformerDecoderLayer\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (encoder_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "            (5): RecursiveScriptModule(\n",
       "              original_name=TransformerDecoderLayer\n",
       "              (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (activation_dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "              (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (encoder_attn): RecursiveScriptModule(\n",
       "                original_name=MultiheadAttention\n",
       "                (dropout_module): RecursiveScriptModule(original_name=FairseqDropout)\n",
       "                (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "                (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "              (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "              (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "              (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            )\n",
       "          )\n",
       "          (output_projection): RecursiveScriptModule(original_name=Linear)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "maritime-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__dataclass',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_generation_fast',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_args',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'args',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'build_decoder',\n",
       " 'build_embedding',\n",
       " 'build_encoder',\n",
       " 'build_model',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decoder',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'encoder',\n",
       " 'eval',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'extract_features',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'forward_decoder',\n",
       " 'from_pretrained',\n",
       " 'get_normalized_probs',\n",
       " 'get_normalized_probs_scriptable',\n",
       " 'get_targets',\n",
       " 'half',\n",
       " 'hub_models',\n",
       " 'load_state_dict',\n",
       " 'make_generation_fast_',\n",
       " 'max_decoder_positions',\n",
       " 'max_positions',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'output_layer',\n",
       " 'parameters',\n",
       " 'prepare_for_inference_',\n",
       " 'prepare_for_onnx_export_',\n",
       " 'prepare_for_tpu_',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'set_num_updates',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_align_args',\n",
       " 'to',\n",
       " 'train',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'upgrade_args',\n",
       " 'upgrade_state_dict',\n",
       " 'upgrade_state_dict_named',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(decoder._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-latvia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_neuron_env",
   "language": "python",
   "name": "torch_neuron_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
